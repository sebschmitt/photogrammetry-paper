\section{Rekonstruktion}

In diesem Abschnitt werden die einzeln Schritte zum Rekonstruieren der Korrespondenzen erklärt, die im vorherigen Prozess gebildet worden sind. 
Für die Rekonstruktion wird die Klasse \emph{SceneReconstructor} in der Datei \emph{reconstruction.hpp} definiert und in \emph{reconstruction.cpp} implementiert. 
Die Klasse besitzt einen Konstruktor, welche eine Instanz der Calibration Klasse als Argument benötigt. 
Mit der Instanz wird die private Member-Variable \emph{calbiration} instanziiert, so dass sie später bei manchen Berechnungen verwendet werden kann.
Des Weiteren wird öffentlich die Methode \emph{reconstructScenes} definiert.
Diese Methode erfordert eine Instanz der Iterator-Klasse mit dem ImagePair als Template-Type. %TODO: check spelling
Mit dem Iterator werden in der Methode die korrespondierenden Bildpunkte rekonstruiert.
Dabei wird darauf geachtet, dass die rekonstruierten Punkte der einzelnen Paare später auch zusammen passen.
Wie in XXX zusehen ist, werden einige weitere private Methoden für die Klasse definiert.
Diese werden in den folgend Paragraphen erläutert, wenn sie für bestimmte Berechnungen genutzt werden.

Die Rekonstruktion wird mit den folgenden 5 Schritten durchgeführt:

\begin{enumerate}
    \item Lokale Rotation \& Translation bestimmen
    \item globale Translation berechnen
    \item Projektionsmatrix berechnen
    \item Triangulation
    \item Skalieren
        \begin{enumerate}
            \item skalierte globale Translation berechnen
            \item Projektionsmatrix berechnen
            \item Triangulation
        \end{enumerate}
\end{enumerate}

Im ersten Schritt muss für ein Bildpaar die Rotation und Translation der einen Kamera bestimmt werden.
Dazu kann wie in \cref{sec:essential-matrix} beschrieben wird, die essentielle Matrix berechnet und in $R$ und $t$ zerlegt werden.
OpenCV bietet dafür die Funktionen \emph{findFundamentalMat} und \emph{recoverPose} an.
In Zeile XXX des Listings XXX wird findFundamentalMat aufgerufen. 
Hier bei werden die korrespondierenden Bildpunkte der beiden Bilder und die Kalibrierungsmatrix übergeben.
Mit den weiteren drei Argumenten wird der angewendete Algorithmus gewählt und angepasst.
Mit dem Literal \emph{cv::RANSAC} wird der Ransac Algorithmus gewählt und so angepasst, das mit 99.9\% Wahrscheinlichkeit die bestimmte Matrix korrekt ist.
Als Threshold für Punkte, die innerhalb der essentielle Matrix passen, wir 1 Pixel verwendet. 
Neben Ransac kann auch LMED verwendet werden.
Der Grundlegende Unterschied der beiden Algorithmen ist XXX.
Es wird Ransac verwendet weil XXX.
Mit dem Mask Argument, wird in einem Vektor vermerkt, welche der Matches Ausreißer sind.
Die somit bestimmte essentielle Matrix wird dann in Zeile XXX mit recoverPose zerlegt.
Dazu werden neben der essentiellen Matrix auch die wieder die korrespondierenden Bildpunkte und die Kalibrierungsmatrix angegeben.
% erkläre kurz die Methode

Die zuvor berechnete Rotationsmatrix und Translationsvektor beschreiben nur die Ausrichtung und der Verschiebung der rechten Kamera im Vergleich zur linken Kamera.
Die Punkte, die mit dieser Pose rekonstruiert werden, werden in einem Koordinatensystem beschrieben, dessen Zentrum und Ausrichtung durch die linke Kamera definiert wird.
Dies gilt für jedes Bildpaar in der Sequenz, d.h.\ jedes Bildpaar definiert ihr eigenes Koordinatensystem.
Damit alle rekonstruierten Bildpunkte im gleichen Koordinatensystem dargestellt werden, müssen $R$ und $t$ transformiert werden.
Die Transformation des nten Bildpaars kann die mit 
\[ T_n = T_{n-1} \cdot \begin{bmatrix}
            R & t \\
            0 & 1 \\
        \end{bmatrix}
    \] 
berechnet werden.
Dabei ist $T_0$ die $4 \times 4$ Identitätsmatrix.
Die Transformationsmatrix $T_n$ kann nun in $R_n$ und $t_n$ aufgeteilt werden. 
$R_n$ und $t_n$ sind hierbei die Rotation und Translation der rechten Kamera des nten Bildpaares im Bezug auf die linke Kamera des ersten Paars.
Es gilt $T_n = \begin{bmatrix}
                R_n & t_n \\
                0 & 1 \\
            \end{bmatrix}$
Für diesen Schritt wird in XXX Zeile XXX die Methode \emph{combineToTranformation} implementiert.
Sie nimmt als Argumente eine $3 \times 3$ und eine $1\times3$ Matrix an und erstellt mit ihnen eine 
$4\times4$ Transformationsmatrix.
Die Transformationsmatrix des vorherigen Bildpaares $T_{n-1}$ wird von der Methode \emph{getPreviousTransform} der ImagePair Klasse bereit gestellt.
Ihre Implementation kann in XXX nachgeschaut werden.
Die Methode \emph{getFromTransformation} in XXX ist eine Hilfsmethode zum Extrahieren der Rotation und Translation einer $4\times4$ Transformationsmatrix.

Mit $R_n$ und $t_n$ wird in Schritt 4 mit der Formel XXX die Projektionsmatrix berechnet.
Die Berechnung wird durch die Methode \emph{getProjection} durchgeführt.
Wie in XXX zu sehen ist, werden nur die Rotation und Translation als Argumente benötigt.
Die Kalibrierungsmatrix $K$ wird durch die Instanz der Calibraion Klasse bereitgestellt. 

Die Schritte 2 und 3 werden in einer Methode zusammengefasst, da sie im vierten Schritt wiederholt werden müssen.
Dazu wird in XXX die Methode \emph{computePoseAndProjection} implementiert, die $T_n$ und die Projektionsmatrix zurück gibt.

In Schritt 4 werden die Bildpunkte mit der Methode \emph{triangulatePoints} von Opencv rekonstruiert.
Sie benötigt als Argumente die Projektionsmatrizen beider Kameras, sowie die Korrespondenzen aus beiden Bilder.  
Zurückgegeben wird eine $4\times N$ Opencv Matrix, die die rekonstruierten Punkte in homogenen Koordinaten repräsentiert.
Hier kommt noch was genaueres

Die Rekonstruktion des ersten Bildpaares ist mit diesem Schritt bereits vollendet.
Durch die Methode \emph{setReconstruction} werden die Ergebnisse der Rekonstruktion für die aktuelle ImagePair Instanz gespeichert.
Hier auch

Anschließend muss für alle Bildpaaren (bis auf das Erste) eine Skalierung vorgenommen werden.
Dies liegt daran, dass die Rekonstruktion nur maßstabsgetreu ist.
Hier sowieso
